\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[russian]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Краткое описание основных шагов метода TOPSIS}
\author{Александр Владимирович Демидовский}

\begin{document}
\maketitle

\begin{abstract}
Данный документ содержит конспект основных шагов метода многокритериального анализа решений TOPSIS. Даный документ носит ознакомительный характер и выполнен в рамках подготовки диссертации на соискание степени кандидата компьютерных наук.
\end{abstract}

\section{Введение}

    Метод TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) был предложен
    исследователями Yoon and Hwang \cite{scm:electre:4:hwang1981topsis}. Общая идея заключается
    в том, что после определения «идеального» и «негативно-идеального» решения происходит
    попытка определения такой альтернативы, которая будет одновременно ближайшей к «идеальному»
    и максимально удаленной от «негативно-идеального» решения. Как и обычно, процесс принятия
    решения начинается с оценки альтернатив по критериям. В результате получается матрица
    решений \(A\) (\ref{eq:decision_matrix}). Рассмотрим процесс принятия решения с момента, когда
    начинается сбор оценок экспертов по каждой альтернативе по каждому критерию. В результате
    формируется матрица A, которая имеет следующую форму:

    \begin{equation}
    \label{eq:decision_matrix}
        A = \begin{bmatrix}
            x_{11}&  x_{12}& \hdotsfor{4} & x_{1n}\\
            x_{21}&  x_{22}& \hdotsfor{4} & x_{2n}\\
        \hdotsfor{7}\\
        x_{m1}&  x_{m2}& . & . & . & . & x_{mn}\\
        \end{bmatrix}
    \end{equation}

    где \(x_{ij}\) обозначает оценку, данную по \(i\)-той альтернативе и по \(j\)-тому критерию.

    Оригинальный метод состоит из 6 шагов
    \cite{scm:electre:1:hwang1981methods}, каждый из которых рассмотрен ниже.

\section{Описание рассматриваемого метода: TOPSIS}
    \subsection{Расчет нормализованной матрицы решений}
        Алгоритм расчета соответствует аналогичному шагу в методе ELECTRE I.

        Во время данного шага нормализация происходит по столбцу в силу того, что столбец
        соответствует заданному критерию и ему соответствует одна единица измерения.

        \begin{equation}
            \label{eq:norm_decision_matrix}
            r_{ij} = \frac{x_{ij}}{\sqrt{\sum_{i=1}^{m}x_{ij}^2}}
        \end{equation}

        В результате получаем нормализованную матрицу решений \(R\).

        \[
            R = \begin{bmatrix}
             r_{11}&  r_{12}& \hdotsfor{4} & r_{1n}\\
             r_{21}&  r_{22}& \hdotsfor{4} & r_{2n}\\
            \hdotsfor{7}\\
            r_{m1}&  r_{m2}& . & . & . & . & r_{mn}\\
            \end{bmatrix}
        \]

    \subsection{Расчет взвешенной нормализованной матрицы решений}
        Веса \(w=(w_1,w_2,\dots,w_n)\) назначаются каждому критерию так, что требуется умножить
        каждую \(i\)-тый столбец нормализованной матрицы решения на \(j\)-тый вес.
        Для сохранения матричной формы расчетов, представим веса как диагональную матрицу \(W\):

        \[
            W = \begin{bmatrix}
                w_{1}&  0 & \hdotsfor{2} & 0\\
                0 &  w_{2}& \hdotsfor{2} & 0\\
                \hdotsfor{5}\\
                0 &  0 & . & . & w_{n}\\
            \end{bmatrix}
        \]

        Затем, взвешенная нормализованная матрица решений получается в результате матричного
        умножения нормализованной матрицы решений и диагональной матрицы весов:

        \begin{equation}
        \label{eq:weighted_norm_decision_matrix}
            \begin{alignedat}{2}
                V &= \begin{bmatrix}
                    r_{11}&  r_{12}& \hdotsfor{4} & r_{1n}\\
                    r_{21}&  r_{22}& \hdotsfor{4} & r_{2n}\\
                    \hdotsfor{7}\\
                    r_{m1}&  r_{m2}& . & . & . & . & r_{mn}\\
                \end{bmatrix} * \begin{bmatrix}
                    w_{1}&  0 & \hdotsfor{2} & 0\\
                    0 &  w_{2}& \hdotsfor{2} & 0\\
                    \hdotsfor{5}\\
                    0 &  0 & . & . & w_{n}\\
                \end{bmatrix} \\
                &= \begin{bmatrix}
                    r_{11}*w_{1}&  r_{12} * w_{2}& \hdotsfor{4} & r_{1n}*w_{n}\\
                    r_{21}*w_{1}&  r_{22} * w_{2}& \hdotsfor{4} & r_{2n}*w_{n}\\
                    \hdotsfor{7}\\
                    r_{m1}*w_{1}&  r_{m2} * w_{2}& . & . & . & . & r_{mn}*w_{n}\\
                \end{bmatrix}
                \end{alignedat}
        \end{equation}

    \subsection{Определение «идеального» и «негативно-идеального» решений}
        Важно отметить, что критерии могут иметь различный смысл. Часть из них направлена на
        максимизацию значений (на выигрыш), другая - на сокращение (на затраты). Поэтому это
        учитывается при построении «идеального» \(A^{*}\) и «негативно-идеального» \(A^{-}\)
        решений:

        \begin{equation}
            \begin{split}
                A^{*} & = \{ (\max_{i} v_{ij} \mid j \epsilon J^{benefit}),
                (\min_{i} v_{ij} \mid j \epsilon J^{cost}) \mid i = 1, 2, \dots, m\} \\
                A^{-} & = \{ (\min_{i} v_{ij} \mid j \epsilon J^{benefit}),
                (\max_{i} v_{ij} \mid j \epsilon J^{cost}) \mid i = 1, 2, \dots, m\}
            \end{split}
        \end{equation}

        \(J^{benefit}\) обозначает подмножество критериев, направленных на максимизацию
        значений, \(J^{cost}\) - на минимизацию. Вместе они формируют полное множество
        критериев \(J\).

    \subsection{Расчет метрики различия}
        Для того, чтобы сравнивать векторы оценок по заданным альтернативам с «идеальным» и
        «негативно-идеальным» решениями, требуется использование некоторой метрики расстояния.
        В оригинальной работе в качестве метрики используется евклидово расстояние.
        Другими словами, для каждой
        \(i\)-той альтернативы рассчитывается ее расстояние до «идеального» решения
        (\(S_{i^{*}}\)) и до «негативно-идеального» (\(S_{i^{-}}\)):

        \begin{equation}
            \begin{split}
                S_{i^{*}} & = \sqrt{\sum_{j=1}^{n}{(v_{ij} - v_{j}^{*})^2}},
                i = 1, 2, \dots, m \\
                S_{i^{-}} & = \sqrt{\sum_{j=1}^{n}{(v_{ij} - v_{j}^{-})^2}},
                i = 1, 2, \dots, m
            \end{split}
        \end{equation}

    \subsection{Расчет относительной близости к «идеальному» решению}
        Для каждой альтернативы \(A_i\) близость рассчитывается следующим образом:

        \begin{equation}
            C_{i^{*}} = \frac{S_{i^{-}}}{S_{i^{-}} + S_{i^{*}}}
        \end{equation}

        Чем ближе \(C_{i^{*}}\) к 1, тем ближе альтернатива \(A_i\) к «идеальному» решению.

    \subsection{Ранжирование в порядке предпочтительности}
        Так как каждая альтернатива имеет соответствующую метрику близости, нужно осуществить
        сортировку всех альтернатив по этому критерию в порядке убывания и выбрать те
        альтернативы, которые имеют максимальное значение как лучшие.

\bibliographystyle{alpha}
\bibliography{sample}

\end{document}
