from benchmark.task.generator.task_type import TaskType
from benchmark.task.schemas.task_scheme import ScalesDescription

available_scales = {
    3: {
        'labels': ['p', 'm', 'g'],
        'values': [1, 3, 5]
    },
    5: {
        'labels': ['vp', 'p', 'm', 'g', 'vg'],
        'values': [1, 3, 5, 7, 9]
    },
    7: {
        'labels': ['vp', 'p', 'mp', 'm', 'mg', 'g', 'vg'],
        'values': [1, 3, 5, 7, 9, 11, 13]
    },
    9: {
        'labels': ['w', 'vp', 'p', 'mp', 'm', 'mg', 'g', 'vg', 'a'],
        'values': [1, 3, 5, 7, 9, 11, 13, 15, 17]
    }
}


def _new_scale(granularity, has_values: bool = False):
    values = None if not has_values else available_scales[granularity]['values']
    return ScalesDescription(scaleID=f's{granularity}',
                             scaleName=f'Autogenerated Scale {granularity}',
                             labels=available_scales[granularity]['labels'],
                             values=values)


def generate_scales(task_type: TaskType):
    # ML-LDM has hardcoded scale 7 as a target therefore it is mandatory to put it here
    scales: list[ScalesDescription] = [_new_scale(7)]

    if task_type is TaskType.NUMERIC_ONLY:
        return scales

    if task_type is TaskType.HYBRID_CRISP_LINGUISTIC:
        scales.append(_new_scale(5, has_values=True))
        return scales

    if task_type is TaskType.HYBRID_FUZZY_LINGUISTIC:
        scales.append(_new_scale(3))
        scales.append(_new_scale(5))
        scales.append(_new_scale(9))
        return scales
